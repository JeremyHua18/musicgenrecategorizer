{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import thecypher as cy\n",
    "import urllib.request as urllib2\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "from unidecode import unidecode\n",
    "from urllib.request import urlopen\n",
    "import requests\n",
    "import time\n",
    "import lyricsgenius\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import Request, urlopen\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import logging\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics.pairwise import cosine_similarity,cosine_distances\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression,LinearRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn import svm\n",
    "import sklearn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pop' 'death-metal' 'rock' 'r&b/soul' 'hip-hop/rap' 'alternative' 'metal'\n",
      " 'punk' 'pop/rock']\n",
      "      Unnamed: 0        artist                  song  \\\n",
      "853          853     Metallica     The Unforgiven II   \n",
      "854          854     Metallica         My Apocalypse   \n",
      "855          855     Metallica     Spit Out the Bone   \n",
      "856          856     Metallica  Nothing Else Matters   \n",
      "857          857     Metallica       Hero of the Day   \n",
      "...          ...           ...                   ...   \n",
      "3014        3014  Machine Head         From This Day   \n",
      "3015        3015  Machine Head            Slanderous   \n",
      "3016        3016  Machine Head                  Halo   \n",
      "3017        3017  Machine Head            Now We Die   \n",
      "3018        3018  Machine Head             Game Over   \n",
      "\n",
      "                                                 lyrics  genre  lyric_count  \\\n",
      "853   lay beside tell done speak words hear make dem...  metal          105   \n",
      "854   claustrophobic crawl skin heart explosive reac...  metal          123   \n",
      "855   unto feel perfection unto dedicate unto never ...  metal           87   \n",
      "856   close matter far could much heart forever trus...  metal           56   \n",
      "857   pain run stays right side tear open things ins...  metal           77   \n",
      "...                                                 ...    ...          ...   \n",
      "3014  go 1 2 3 4 yeah yeah yeah got stand tall cut a...  metal          114   \n",
      "3015  wasted ugly underbelly fat fuck fairy reject w...  metal           58   \n",
      "3016  call arms stand beside time fight compromising...  metal           62   \n",
      "3017  standing edge world uncertainty calling page u...  metal          114   \n",
      "3018  oh sweet little lies hear hold close look eyes...  metal          139   \n",
      "\n",
      "      lyric_count_norm  \n",
      "853           9.929144  \n",
      "854           6.506470  \n",
      "855           5.998152  \n",
      "856           3.897104  \n",
      "857           7.658657  \n",
      "...                ...  \n",
      "3014          8.675293  \n",
      "3015          4.303758  \n",
      "3016          4.473198  \n",
      "3017          8.539741  \n",
      "3018         11.115219  \n",
      "\n",
      "[304 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "df_wordCount = pd.read_csv(\"data.csv\")\n",
    "\n",
    "print(df_wordCount.genre.unique())\n",
    "\n",
    "\n",
    "subset = df_wordCount[df_wordCount.genre=='metal']\n",
    "\n",
    "print(subset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "462\n",
      "341\n",
      "441\n",
      "380\n",
      "297\n",
      "358\n",
      "303\n",
      "352\n",
      "348\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "df_wordCount.dropna(\n",
    "    axis=0,\n",
    "    how='any',\n",
    "    thresh=None,\n",
    "    subset=None,\n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "genres = ['pop','death-metal','rock','r&b/soul','hip-hop/rap','alternative','metal','punk','pop/rock',]\n",
    "\n",
    "\n",
    "N = 290 # number of records to pull from each genre\n",
    "RANDOM_SEED = 200 # random seed to make results repeatable\n",
    "\n",
    "train_df = pd.DataFrame()\n",
    "test_df = pd.DataFrame()\n",
    "for g in genres: # loop over each genre\n",
    "    subset = df_wordCount[df_wordCount.genre == g] # create subset\n",
    "    print(len(subset))\n",
    "    train_set = subset.sample(n=N, random_state=RANDOM_SEED)\n",
    "    test_set = subset.drop(train_set.index)\n",
    "    train_df = train_df.append(train_set) # append subsets to the master sets\n",
    "    test_df = test_df.append(test_set)\n",
    "    \n",
    "train_df = shuffle(train_df)\n",
    "test_df = shuffle(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pop' 'r&b/soul' 'metal' 'pop/rock' 'alternative' 'punk' 'hip-hop/rap'\n",
      " 'death-metal' 'rock']\n",
      "      Unnamed: 0               artist  \\\n",
      "95            95           Ed Sheeran   \n",
      "3195        3195         Mary J Blige   \n",
      "1299        1299             Megadeth   \n",
      "2786        2786      Imagine Dragons   \n",
      "1545        1545  Stone Temple Pilots   \n",
      "2947        2947        The Offspring   \n",
      "3167        3167            blink 182   \n",
      "2962        2962            Buzzcocks   \n",
      "1655        1655        Black Sabbath   \n",
      "1252        1252              Stormzy   \n",
      "\n",
      "                                                   song  \\\n",
      "95                       Give Me Love The Parting Glass   \n",
      "3195                                      JUST STAND UP   \n",
      "1299                        Dread and the Fugitive Mind   \n",
      "2786                                       I m So Sorry   \n",
      "1545                                          Meat Plow   \n",
      "2947                                      Have You Ever   \n",
      "3167                                       Wishing Well   \n",
      "2962  Ever Fallen In Love With Someone You Shouldn t ve   \n",
      "1655                                 Fairies Wear Boots   \n",
      "1252                           Rachael s Little Brother   \n",
      "\n",
      "                                                 lyrics        genre  \\\n",
      "95    give love like lately waking alone paint splat...          pop   \n",
      "3195  Everything alright yeah The heart stronger thi...     r&b/soul   \n",
      "1299  let introduce social disease wealth leave knee...        metal   \n",
      "2786  time anyone telling deeds sign roaring thunder...     pop/rock   \n",
      "1545  fine place day full breakdowns takes meltdown ...  alternative   \n",
      "2947  falling falling ever walked room like room pas...         punk   \n",
      "3167  gone long time kinda lost way find caught shor...         punk   \n",
      "2962  spurn natural emotions make feel dirt hurt sta...         punk   \n",
      "1655  going home late last night suddenly got fright...        metal   \n",
      "1252  love never always partpart like father help pa...  hip-hop/rap   \n",
      "\n",
      "      lyric_count  lyric_count_norm  \n",
      "95             94          7.218115  \n",
      "3195          141          9.929144  \n",
      "1299           59          5.591497  \n",
      "2786           68          5.828712  \n",
      "1545           62          4.642637  \n",
      "2947          101          7.861984  \n",
      "3167           82          6.133703  \n",
      "2962           45          3.998768  \n",
      "1655           44          3.253235  \n",
      "1252          486         36.260012  \n"
     ]
    }
   ],
   "source": [
    "print(train_df.genre.unique())\n",
    "print(train_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train a model using word frequencies and sklearn's CountVectorizer. The CountVectorizer is a quick and dirty way to train a language model by using simple word counts. Later we'll try a more sophisticated approach with the TfidfVectorizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.45982142857142855\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# define our model\n",
    "wordCount_clf = Pipeline(\n",
    "    [('vect', CountVectorizer()),\n",
    "     ('clf', MultinomialNB(alpha=0.1))])\n",
    "\n",
    "# train our model on training data\n",
    "wordCount_clf.fit(train_df.lyrics, train_df.genre)  \n",
    "\n",
    "# score our model on testing data\n",
    "predicted = wordCount_clf.predict(test_df.lyrics)\n",
    "print(np.mean(predicted == test_df.genre))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49404761904761907\n"
     ]
    }
   ],
   "source": [
    "# define our model\n",
    "wordCount_clf = Pipeline(\n",
    "    [('vect', TfidfVectorizer()),\n",
    "     ('clf', BernoulliNB(alpha=0.1))])\n",
    "\n",
    "# train our model on training data\n",
    "wordCount_clf.fit(train_df.lyrics, train_df.genre)  \n",
    "\n",
    "# score our model on testing data\n",
    "predicted = wordCount_clf.predict(test_df.lyrics)\n",
    "print(np.mean(predicted == test_df.genre))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47023809523809523"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# define our model\n",
    "text_clf = Pipeline(\n",
    "    [('vect', TfidfVectorizer()),\n",
    "     ('clf', LogisticRegression(solver=\"liblinear\", multi_class=\"ovr\"))])\n",
    "\n",
    "# train our model on training data\n",
    "text_clf.fit(train_df.lyrics, train_df.genre)  \n",
    "\n",
    "# core our model on testing data\n",
    "predicted = text_clf.predict(test_df.lyrics)\n",
    "np.mean(predicted == test_df.genre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3020833333333333"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define our model\n",
    "text_clf = Pipeline(\n",
    "    [('vect', TfidfVectorizer()),\n",
    "     ('clf', DecisionTreeClassifier())])\n",
    "\n",
    "# train our model on training data\n",
    "text_clf.fit(train_df.lyrics, train_df.genre)  \n",
    "\n",
    "# core our model on testing data\n",
    "predicted = text_clf.predict(test_df.lyrics)\n",
    "np.mean(predicted == test_df.genre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46726190476190477"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define our model\n",
    "text_clf = Pipeline(\n",
    "    [('vect', TfidfVectorizer()),\n",
    "     ('clf', MLPClassifier())])\n",
    "\n",
    "# train our model on training data\n",
    "text_clf.fit(train_df.lyrics, train_df.genre)  \n",
    "\n",
    "# core our model on testing data\n",
    "predicted = text_clf.predict(test_df.lyrics)\n",
    "np.mean(predicted == test_df.genre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44804318488529016"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define our model\n",
    "text_clf = Pipeline(\n",
    "    [('vect', TfidfVectorizer()),\n",
    "     ('clf', RandomForestClassifier(n_estimators=100,max_features=\"sqrt\"))])\n",
    "\n",
    "# train our model on training data\n",
    "text_clf.fit(train_df.lyrics, train_df.genre)  \n",
    "\n",
    "# core our model on testing data\n",
    "predicted = text_clf.predict(test_df.lyrics)\n",
    "np.mean(predicted == test_df.genre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48110661268556004"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define our model\n",
    "text_clf = Pipeline(\n",
    "    [('vect', TfidfVectorizer()),\n",
    "     ('clf', MultinomialNB(alpha=0.1))])\n",
    "\n",
    "# train our model on training data\n",
    "text_clf.fit(train_df.lyrics, train_df.genre)  \n",
    "\n",
    "# core our model on testing data\n",
    "predicted = text_clf.predict(test_df.lyrics)\n",
    "np.mean(predicted == test_df.genre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - '/Users/honghaozhu/nltk_data'\n    - '/Users/honghaozhu/opt/anaconda3/nltk_data'\n    - '/Users/honghaozhu/opt/anaconda3/share/nltk_data'\n    - '/Users/honghaozhu/opt/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet.zip/wordnet/\u001b[0m\n\n  Searched in:\n    - '/Users/honghaozhu/nltk_data'\n    - '/Users/honghaozhu/opt/anaconda3/nltk_data'\n    - '/Users/honghaozhu/opt/anaconda3/share/nltk_data'\n    - '/Users/honghaozhu/opt/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-1b6a03da0963>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# train our model on training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mtext_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlyrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenre\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# score our model on testing data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0mThis\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \"\"\"\n\u001b[0;32m--> 350\u001b[0;31m         \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m         with _print_elapsed_time('Pipeline',\n\u001b[1;32m    352\u001b[0m                                  self._log_message(len(self.steps) - 1)):\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    313\u001b[0m                 \u001b[0mmessage_clsname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Pipeline'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m                 \u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m                 **fit_params_steps[name])\n\u001b[0m\u001b[1;32m    316\u001b[0m             \u001b[0;31m# Replace the transformer of the step with the fitted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m             \u001b[0;31m# transformer. This is necessary when loading the transformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/joblib/memory.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[1;32m    726\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fit_transform'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    729\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1857\u001b[0m         \"\"\"\n\u001b[1;32m   1858\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1859\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1860\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1861\u001b[0m         \u001b[0;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m-> 1220\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m   1221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1222\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1129\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1131\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1132\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1133\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mngrams\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstop_words\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m                 \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mngrams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m                 \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mngrams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_word_ngrams\u001b[0;34m(self, tokens, stop_words)\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0;31m# handle stop words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstop_words\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m             \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0;31m# handle token n-grams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0;31m# handle stop words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstop_words\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m             \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0;31m# handle token n-grams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-1b6a03da0963>\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mwnl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misalnum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# only words that are  2 characters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     )                                 # and is alpha-numeric\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/nltk/stem/wordnet.py\u001b[0m in \u001b[0;36mlemmatize\u001b[0;34m(self, word, pos)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNOUN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mlemmas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwordnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_morphy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemmas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlemmas\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     86\u001b[0m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;31m# Load the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m                 \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    699\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'*'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - '/Users/honghaozhu/nltk_data'\n    - '/Users/honghaozhu/opt/anaconda3/nltk_data'\n    - '/Users/honghaozhu/opt/anaconda3/share/nltk_data'\n    - '/Users/honghaozhu/opt/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stop = list(set(stopwords.words('english'))) # stopwords\n",
    "wnl = WordNetLemmatizer() # lemmatizer\n",
    "\n",
    "def tokenizer(x): # custom tokenizer\n",
    "    return (\n",
    "        wnl.lemmatize(w) \n",
    "        for w in word_tokenize(x) \n",
    "        if len(w)==2 and w.isalnum() # only words that are  2 characters\n",
    "    )                                 # and is alpha-numeric\n",
    "\n",
    "# define our model\n",
    "text_clf = Pipeline(\n",
    "    [('vect', TfidfVectorizer(\n",
    "        ngram_range=(1, 2), # include bigrams\n",
    "        tokenizer=tokenizer,\n",
    "        stop_words=stop,\n",
    "        max_df=0.4, # ignore terms that appear in more than 40% of documents\n",
    "        min_df=4)), # ignore terms that appear in less than 4 documents\n",
    "     ('tfidf', TfidfTransformer()),\n",
    "     ('clf', MultinomialNB(alpha=0.1))])\n",
    "\n",
    "# train our model on training data\n",
    "text_clf.fit(train_df.lyrics, train_df.genre)  \n",
    "\n",
    "# score our model on testing data\n",
    "predicted = text_clf.predict(test_df.lyrics)\n",
    "np.mean(predicted == test_df.genre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Well the accuracy is not high, but good start**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Unnamed: 0         0         1         2         3         4         5  \\\n",
      "0        pop -0.034629 -0.067963  0.069235 -0.042588 -0.065489  0.029289   \n",
      "1        pop -0.061408 -0.069197  0.061201  0.050199 -0.019850 -0.033546   \n",
      "2        pop -0.039916 -0.044595  0.013519 -0.058858 -0.032305 -0.052050   \n",
      "3        pop -0.001811 -0.000001  0.024851  0.030089 -0.024987 -0.062170   \n",
      "4        pop -0.001963 -0.039135 -0.011527  0.025387 -0.006315  0.007541   \n",
      "5        pop -0.059922 -0.061264  0.062284  0.059680 -0.052203 -0.047218   \n",
      "6        pop -0.007013 -0.072376 -0.066359  0.038720 -0.054151 -0.046476   \n",
      "7        pop  0.004870 -0.055279  0.061035  0.007988 -0.012775 -0.052248   \n",
      "8        pop  0.007114 -0.059523  0.058985  0.050068 -0.036469 -0.058044   \n",
      "9        pop -0.021379 -0.063510  0.029503  0.061463 -0.036552 -0.008131   \n",
      "\n",
      "          6         7         8  ...       502       503       504       505  \\\n",
      "0  0.044530 -0.021600 -0.025340  ...  0.001767 -0.068704  0.042984 -0.046591   \n",
      "1  0.068392 -0.043247 -0.041831  ... -0.021599 -0.069697 -0.041149 -0.035933   \n",
      "2  0.037210 -0.053616 -0.064471  ... -0.026310 -0.057163  0.016373 -0.037657   \n",
      "3 -0.010401  0.052844 -0.059936  ... -0.024668 -0.063823 -0.041624 -0.018149   \n",
      "4 -0.002482 -0.025721 -0.071047  ... -0.002779 -0.070740  0.015271  0.019762   \n",
      "5 -0.061848 -0.004132 -0.057089  ...  0.059959 -0.062203  0.038835  0.011783   \n",
      "6  0.040780  0.025676 -0.066925  ...  0.012032 -0.075495  0.004703  0.028254   \n",
      "7 -0.006155 -0.041397 -0.059411  ...  0.016953 -0.056453 -0.022323 -0.047079   \n",
      "8  0.019202 -0.048557 -0.029040  ...  0.031679 -0.061700  0.006355  0.044475   \n",
      "9  0.037790  0.053080 -0.060685  ... -0.010468 -0.063358  0.011359  0.049696   \n",
      "\n",
      "        506       507       508       509       510       511  \n",
      "0 -0.031594  0.052764  0.018488 -0.053851 -0.056631  0.009160  \n",
      "1  0.050155 -0.022035 -0.031341  0.050096 -0.063111 -0.019262  \n",
      "2 -0.065257  0.030892  0.001619 -0.037716  0.059352  0.014932  \n",
      "3 -0.008309 -0.057460  0.055945  0.031019 -0.055563  0.024426  \n",
      "4 -0.002895  0.051591 -0.023805 -0.033311 -0.039285  0.009222  \n",
      "5 -0.026296 -0.026468  0.019242 -0.034090 -0.053939 -0.039376  \n",
      "6  0.036356  0.056447 -0.069992  0.018528 -0.043334 -0.028158  \n",
      "7  0.017330  0.030339 -0.029919 -0.004464 -0.009327  0.030167  \n",
      "8 -0.011682 -0.005627 -0.058872 -0.008037 -0.057187  0.023876  \n",
      "9  0.030515  0.024816 -0.056839  0.036961 -0.060387  0.004944  \n",
      "\n",
      "[10 rows x 513 columns]\n",
      "RangeIndex(start=0, stop=3288, step=1)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"lyrcisVec&Genre_only.csv\")\n",
    "print(df.head(10))\n",
    "print(df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.03462914377450943 -0.06796256452798842 0.06923465430736543\n",
      " -0.04258839413523674 -0.06548874080181122 0.029288625344634056\n",
      " 0.04453044757246971 -0.021599812433123592 -0.02533990703523159\n",
      " -0.06388705223798752 0.04576509073376656 0.009967030957341194\n",
      " 0.016217440366745 -0.04426725953817368 0.033411882817745216\n",
      " -0.06873811036348343 -0.06816203147172928 0.05381111055612564\n",
      " 0.015437314286828041 -0.0558173805475235 -0.027449609711766243\n",
      " 0.0040187109261751175 0.05352984741330147 -0.04385388642549515\n",
      " -0.06925974786281586 0.04875700920820236 0.04762130230665207\n",
      " 0.04377871751785278 0.04609393700957298 0.033854592591524124\n",
      " 0.04363492131233215 -0.024636369198560715 0.04799265414476395\n",
      " 0.01922309957444668 -0.05456623435020447 0.06532098352909088\n",
      " -0.0030587064102292057 0.006507018581032753 0.0002682261110749096\n",
      " 0.06889160722494125 0.0134216770529747 0.0033326291013509035\n",
      " -0.04373688623309136 -0.001722192973829806 0.0012402103748172524\n",
      " -0.028333382681012157 0.018782006576657292 0.06445574015378952\n",
      " 0.003076545661315322 -0.01643146015703678 0.05031338706612587\n",
      " 0.06439567357301712 -0.0685240849852562 0.05383921414613724\n",
      " -0.039783239364624016 0.025008361786603928 0.03058563731610775\n",
      " -0.0671629086136818 -0.019502034410834312 -0.0534684881567955\n",
      " 0.027642417699098587 0.014536312781274319 -0.009332932531833649\n",
      " -0.06843163818120956 -0.04890082776546478 -0.011833574622869492\n",
      " -0.015575047582387924 -0.05124757811427117 -0.004792310763150454\n",
      " 0.060842685401439674 -0.008432939648628235 0.03134884312748909\n",
      " 0.04634557664394379 0.020547233521938324 -0.03705177456140518\n",
      " 0.06475964933633804 -0.027800491079688072 -0.03850333020091057\n",
      " -0.05725304037332535 -0.012674729339778423 0.06859701871871947\n",
      " 0.01737857609987259 -0.03375731781125069 0.06082361936569214\n",
      " 0.03871236741542816 0.06566552072763443 0.005835330579429865\n",
      " -0.02919891104102135 -0.0689205676317215 0.035692714154720306\n",
      " 0.0576147548854351 0.015516440384089947 0.055117476731538766\n",
      " 0.06062384322285652 0.021279841661453247 -0.012712881900370121\n",
      " -0.014394864439964294 0.06373114883899689 0.06919299066066742\n",
      " -0.02989065460860729 0.06400655955076218 0.061855841428041465\n",
      " -0.039355508983135216 -0.05509141460061073 -0.012816804461181164\n",
      " 0.06081560254096985 -0.048738408833742135 -0.0026637862902134657\n",
      " 0.03792734816670418 0.033511701971292496 -0.06720475107431413\n",
      " -0.036465369164943695 -0.04284464940428734 0.03671301528811455\n",
      " 0.06502484530210495 -0.01676815189421177 -0.03689498081803322\n",
      " -0.05664099007844925 -0.04554084688425064 -0.018401190638542172\n",
      " 0.03438441827893257 0.0014909671153873205 -0.04421453177928925\n",
      " 0.05076061561703682 0.0643872618675232 -0.0043867737986147395\n",
      " -0.02268762327730656 -0.06926753371953964 -0.015141221694648266\n",
      " -0.05316732823848725 -0.007896488532423973 -0.06910569965839386\n",
      " 0.04378840699791908 -0.046711526811122894 -0.06645435094833374\n",
      " -0.01303527131676674 0.06283868849277496 -0.0020820777863264084\n",
      " -0.022841973230242733 0.04312056675553322 0.046810414642095566\n",
      " 0.06926456838846208 0.03755899146199226 -0.06920596957206726\n",
      " -0.02509176731109619 -0.05304545164108277 -0.05366922914981842\n",
      " -0.06814846396446228 0.008324361406266691 -0.0638817548751831\n",
      " -0.06651759892702103 -0.007640832569450141 0.03904915601015091\n",
      " -0.030202655121684074 -0.053730711340904236 0.04358338564634323\n",
      " -0.011226419359445572 -0.04477223381400108 0.01662089303135872\n",
      " -0.004663701634854078 -0.021303799003362656 -0.06384843587875366\n",
      " -0.019146140664815903 0.021563349291682243 0.036406196653842926\n",
      " 0.01312091201543808 0.0494639053940773 0.043195758014917374\n",
      " 0.0058651985600590715 -0.058534514158964164 0.06525056809186935\n",
      " 0.02663510479032993 0.0218389481306076 0.04423228278756142\n",
      " -0.05149421840906143 0.017517684027552605 -0.021629514172673225\n",
      " -0.0602414496243 0.06299406290054321 0.01507263630628586\n",
      " 0.0014852267922833562 -0.015608668327331543 0.029340442270040512\n",
      " -0.01827283576130867 -0.053716816008090966 -0.024973724037408832\n",
      " 0.05317191779613495 -0.010337244719266891 0.06908882409334183\n",
      " -0.01039053313434124 -0.026382870972156525 0.04709690064191818\n",
      " -0.001963652903214097 -0.0692632719874382 0.051262978464365005\n",
      " -0.06085352972149849 -0.04913365468382835 0.06666681170463562\n",
      " 0.06869298219680786 -0.06801587343215942 -0.03175872191786766\n",
      " -0.024360433220863342 0.008904395624995232 0.007756423670798541\n",
      " -0.002804097719490528 -0.004828439559787512 0.025859948247671127\n",
      " 0.03079936280846596 0.004878215026110411 0.03664872795343399\n",
      " 0.06469833850860596 0.06356357783079147 -0.04401655122637749\n",
      " 0.020636593922972683 -0.036011017858982086 0.0477503202855587\n",
      " -0.016466356813907623 0.027907142415642742 0.045055106282234185\n",
      " 0.04971431940793991 0.04913746565580368 -0.025988653302192688\n",
      " -0.0647905170917511 0.06626269966363907 0.06915582716464996\n",
      " -0.06917501240968704 -0.06693681329488754 -0.0593380555510521\n",
      " 0.02593118697404861 -0.062493614852428436 0.0579618439078331\n",
      " -0.021048953756690025 0.050894856452941895 -0.06926893442869186\n",
      " 0.015481208451092243 -0.06397779285907745 -0.050716567784547806\n",
      " -0.043479092419147485 0.018194522708654404 -0.05339746922254562\n",
      " 0.06595521420240402 0.06500323861837387 -0.001961897360160947\n",
      " -0.013899163343012331 -0.030487623065710068 0.0512932650744915\n",
      " 0.015690689906477928 0.02218014933168888 0.007307942491024733\n",
      " -0.0661967545747757 0.03467467054724693 -0.04348533973097801\n",
      " -0.0172326248139143 -0.0453747883439064 0.054908331483602524\n",
      " -0.069014772772789 0.03277178481221199 0.06926895678043365\n",
      " 0.0019526121905073524 -0.05681930109858512 -0.04846293479204178\n",
      " 0.042321499437093735 -0.06256230920553207 0.05988604947924614\n",
      " 0.01103705819696188 0.014228812418878078 0.06372519582509995\n",
      " -0.01070373598486185 -0.017837099730968475 0.014283232390880585\n",
      " -0.06826440989971161 0.05761154741048812 -0.058598332107067115\n",
      " -0.03732701763510704 -0.021601181477308273 0.05497919768095017\n",
      " 0.048285085707902915 0.023396994918584824 0.06675981730222702\n",
      " 0.009263744577765465 0.04909782111644745 0.058837264776229865\n",
      " -0.029203481972217563 0.06788658350706099 0.00598663417622447\n",
      " -0.06855349987745285 -0.03214346244931221 0.042858205735683434\n",
      " -0.020233409479260445 0.032250456511974335 -0.06677636504173279\n",
      " -0.023850036785006527 0.06922842562198639 -0.05150425434112549\n",
      " -0.005586889106780291 -0.028931850567460064 0.0024637668393552303\n",
      " -0.02328245528042316 0.0017737777670845392 0.002570398850366473\n",
      " -0.060241416096687324 -0.007621315307915211 -0.01181224174797535\n",
      " 0.014218901284039019 0.0360565185546875 0.056182067841291435\n",
      " -0.06906239688396454 -0.013624994084239006 0.03848759084939957\n",
      " -0.006490578409284353 -0.056735426187515266 -0.054133858531713486\n",
      " 0.01842777617275715 0.040815744549036026 0.03334890678524971\n",
      " 0.0456363782286644 -0.0002907759044319391 0.009425146505236627\n",
      " -0.015646519139409062 0.012459614314138891 0.06606951355934143\n",
      " -0.031325627118349075 -0.040500760078430176 0.06920679658651352\n",
      " -0.05050498992204666 -0.062404677271842963 -0.032826196402311325\n",
      " 0.061477862298488624 -0.0008829381549730898 0.02881751768290997\n",
      " -0.06864775717258452 0.06149151548743247 -0.06691606342792511\n",
      " 0.011110687628388405 -0.06017544865608215 -0.04842966049909592\n",
      " -0.0571456104516983 -0.048285815864801414 0.021827142685651783\n",
      " -0.01592952571809292 0.057175755500793464 0.01365399733185768\n",
      " 0.008915288373827934 -0.0510723702609539 -0.008208016864955425\n",
      " -0.06103658676147461 -0.05945545807480813 0.025896627455949783\n",
      " 0.00012742758553940803 0.056573942303657525 -0.002037926809862256\n",
      " 0.05453615263104439 -0.005174905061721803 -0.009854298084974287\n",
      " -0.06875916570425034 0.0011949153849855065 -0.06926684826612473\n",
      " 0.05555187910795212 -0.0288437120616436 0.04992498457431793\n",
      " 0.026478983461856842 0.009226522408425808 0.0636763796210289\n",
      " 0.06918627768754959 -0.01117820292711258 -0.03588149696588516\n",
      " 0.0688072070479393 0.06122142449021339 -0.05460712313652039\n",
      " -0.04839702695608139 -0.05890719965100288 -0.04551419615745544\n",
      " 0.06062876433134079 0.053466003388166435 -0.04463440924882889\n",
      " 0.04888656362891197 0.02872527204453945 -0.005099062342196703\n",
      " -0.007227928377687931 -0.06441282480955124 0.06559248268604279\n",
      " -0.023240095004439357 0.027530154213309288 0.034804746508598335\n",
      " 0.05466052144765854 -0.06434442847967148 0.059480391442775726\n",
      " 0.02276485413312912 -0.04434021562337875 -0.01596326194703579\n",
      " -0.06671670824289322 -0.005281352438032627 0.006126171909272671\n",
      " 0.03592297434806824 -0.06810911744832993 0.05974740162491799\n",
      " -0.021536851301789284 -0.05823344364762306 0.038437958806753166\n",
      " -0.0687795877456665 0.04312274232506752 -0.04067008569836617\n",
      " 0.002473591128364205 0.029867274686694145 0.02210560068488121\n",
      " 0.06247299164533615 0.069058857858181 0.054169774055480964\n",
      " -0.012905178591609 0.05708203464746475 0.021164961159229282\n",
      " 0.039848431944847114 0.038158576935529716 0.06579411774873734\n",
      " -0.004163440782576799 0.03086722642183304 0.0029524799901992087\n",
      " -0.0419437512755394 0.018757108598947525 0.044581834226846695\n",
      " 0.0565328449010849 0.0017597841797396538 0.01958614215254784\n",
      " -0.005154331214725971 -0.06584442406892776 0.06469911336898804\n",
      " -0.0686272382736206 0.06871441751718521 -0.004371604416519404\n",
      " 0.046108812093734734 0.028202572837471962 0.001027882331982255\n",
      " -0.002575061982497573 0.06923423707485199 0.041382256895303726\n",
      " 0.01608869433403015 -0.030651021748781208 0.06417111307382584\n",
      " -0.06066536903381348 0.04507281631231308 0.033316560089588165\n",
      " 0.0608324408531189 -0.05670389533042908 -0.02575330249965191\n",
      " -0.06033133715391159 -0.016987977549433708 0.05464918538928032\n",
      " -0.029159564524888992 -0.03654464706778526 -0.02485356852412224\n",
      " -0.02087569236755371 -0.00911130104213953 0.06098805740475655\n",
      " -0.054712530225515366 0.05707019940018654 -0.0662786066532135\n",
      " -0.06921892613172531 0.06803613901138306 -0.06395596265792848\n",
      " 0.03494663164019585 -0.06547924876213074 -0.028499249368906018\n",
      " -0.02222099713981152 0.0607912540435791 0.048993047326803214\n",
      " -0.01357206515967846 0.00820949487388134 -0.06609397381544113\n",
      " -0.021054023876786232 0.05971981957554817 -0.03683525323867798\n",
      " 0.04609701409935951 0.001294588320888579 0.013004670850932598\n",
      " 0.018988778814673424 0.04547062888741493 -0.014501731842756271\n",
      " 0.03883537277579308 -0.013382167555391787 -0.06924872100353241\n",
      " 0.013109480030834677 0.05876132845878601 0.01847187615931034\n",
      " -0.060583125799894326 -0.0553600937128067 -0.03785654902458191\n",
      " -0.05481168255209923 0.005280047655105591 -0.0319347120821476\n",
      " -0.051120799034833915 0.060124762356281274 4.9367474275641137e-05\n",
      " 0.05978484824299812 0.040341801941394806 -0.00017211900558322668\n",
      " 0.043265487998723984 0.06831526756286621 -0.025540968403220177\n",
      " -0.0016175489872694016 -0.051105454564094537 -0.01783375255763531\n",
      " 0.009142887778580187 0.0017670687520876527 -0.06870430707931519\n",
      " 0.0429842583835125 -0.04659115150570869 -0.03159431368112564\n",
      " 0.052764274179935455 0.01848775520920753 -0.05385095998644829\n",
      " -0.05663145333528519 0.009159649722278118]\n"
     ]
    }
   ],
   "source": [
    "# print(df.mean(axis = 0))\n",
    "\n",
    "genre_mean = np.array(df.mean(axis = 0))\n",
    "\n",
    "a = np.array(df.iloc[0])\n",
    "\n",
    "a = np.delete(a,0)\n",
    "\n",
    "print(a)\n",
    "# print(df.iloc[0].astype(float))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3282\n",
      "3282\n",
      "  Genre     Cosine Similarity\n",
      "0   pop  [0.5782835254070084]\n",
      "1   pop  [0.7214075516920317]\n",
      "2   pop  [0.5137022403699649]\n",
      "3   pop  [0.6415318392028322]\n",
      "4   pop   [0.566566410976598]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "cos_list = []\n",
    "df_cosine = pd.DataFrame([])\n",
    "for i in range(len (df_wordCount)):\n",
    "    a = np.array(df.iloc[i])\n",
    "    a = np.delete(a,0)\n",
    "    cos_sim=cosine_similarity(a.reshape(1,-1),genre_mean.reshape(1,-1))\n",
    "    cos_sim = cos_sim.flatten()\n",
    "    cos_list.append(cos_sim)\n",
    "#     df_cosine = df_cosine.append(pd.DataFrame([cos_sim], index=[df_wordCount.genre[0]]), ignore_index=True)\n",
    "\n",
    "print(len(cos_list))\n",
    "print(len(df_wordCount.genre))\n",
    "\n",
    "df_cosine['Genre'] = df_wordCount.genre\n",
    "df_cosine['Cosine Similarity'] = cos_list\n",
    "\n",
    "print(df_cosine.head())\n",
    "\n",
    "\n",
    "                          \n",
    "                       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Genre      Cosine Similarity\n",
      "0          pop   [0.5782835254070084]\n",
      "1          pop   [0.7214075516920317]\n",
      "2          pop   [0.5137022403699649]\n",
      "3          pop   [0.6415318392028322]\n",
      "4          pop    [0.566566410976598]\n",
      "...        ...                    ...\n",
      "3283  r&b/soul  [0.49355295568162294]\n",
      "3284  r&b/soul   [0.6368299903351874]\n",
      "3285  r&b/soul   [0.6717994257457806]\n",
      "3286  r&b/soul   [0.5686548794267545]\n",
      "3287  r&b/soul   [0.5797789466011247]\n",
      "\n",
      "[3282 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
